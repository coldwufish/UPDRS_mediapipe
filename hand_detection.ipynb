{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import statistics\n",
    "import argparse\n",
    "from scipy import signal\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.fftpack import fft,ifft\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.signal import freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_mosaic(frame, x, y, w, h, neighbor=9):\n",
    "  \"\"\"\n",
    "  马赛克的实现原理是把图像上某个像素点一定范围邻域内的所有点用邻域内左上像素点的颜色代替，这样可以模糊细节，但是可以保留大体的轮廓。\n",
    "  :param frame: opencv frame\n",
    "  :param int x : 马赛克左顶点\n",
    "  :param int y: 马赛克右顶点\n",
    "  :param int w: 马赛克宽\n",
    "  :param int h: 马赛克高\n",
    "  :param int neighbor: 马赛克每一块的宽\n",
    "  \"\"\"\n",
    "  fh, fw = frame.shape[0], frame.shape[1]\n",
    "  if (y + h > fh) or (x + w > fw):\n",
    "    return\n",
    "  for i in range(0, h - neighbor, neighbor): # 关键点0 减去neightbour 防止溢出\n",
    "    for j in range(0, w - neighbor, neighbor):\n",
    "      rect = [j + x, i + y, neighbor, neighbor]\n",
    "      color = frame[i + y][j + x].tolist() # 关键点1 tolist\n",
    "      left_up = (rect[0], rect[1])\n",
    "      right_down = (rect[0] + neighbor - 1, rect[1] + neighbor - 1) # 关键点2 减去一个像素\n",
    "      cv2.rectangle(frame, left_up, right_down, color, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Read video with OpenCV.\n",
    "cap=cv2.VideoCapture('/home/johnson/Desktop/UPDRS/UPDRS_video/Tim_Tremor/t008_crop_256.mp4')\n",
    "\n",
    "## Get video info\n",
    "RES=(round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "fps = round(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "#create video writer to write detected_video\n",
    "#output_video = \"../UPDRS_result/t014_tremor.mp4\"\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "#out = cv2.VideoWriter(output_video, fourcc, 30, RES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store each frame in video\n",
    "frame_count = 0\n",
    "img_list = []\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        frame_count+=1\n",
    "        img_list.append(frame)\n",
    "\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#edit video to remove extra hands\n",
    "frame_count = 0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        if 30 <= frame_count <= 300:\n",
    "            out.write(frame)\n",
    "        frame_count+=1\n",
    "\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single hand (grasp)\n",
    "hands = mp_hands.Hands(static_image_mode=True,max_num_hands=2,min_detection_confidence=0.7)\n",
    "\n",
    "record = []\n",
    "frame_count = 0\n",
    "img_list = []\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        frame = frame[:,:128]\n",
    "        frame_count += 1\n",
    "\n",
    "        # Convert the BGR image to RGB, flip the image around y-axis for correct \n",
    "        # handedness output and process it with MediaPipe Hands.\n",
    "        results = hands.process(cv2.flip(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), 1))\n",
    "        annotated_image = cv2.flip(frame.copy(), 1)\n",
    "\n",
    "        if results.multi_hand_landmarks != None:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                middle_y = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP].x\n",
    "                record.append(middle_y)\n",
    "\n",
    "                #after processing the hand, flip back the image\n",
    "                annotated_image = cv2.flip(annotated_image, 1)\n",
    "                img_list.append(annotated_image)\n",
    "                #out.write(annotated_image)\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "#out.release()\n",
    "plt.plot(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple hand (pronation)\n",
    "hands = mp_hands.Hands(static_image_mode=True,max_num_hands=4,min_detection_confidence=0.5)\n",
    "\n",
    "history = {} #record which frame has detected result, 0 for detect fail, 1 for detect one hand, 2 for detect two hands\n",
    "thumb_dis = []\n",
    "thumb_list = []\n",
    "frame_count = 0\n",
    "img_list = []\n",
    "results = 0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "\n",
    "        history[str(frame_count)] = 0\n",
    "        # Convert the BGR image to RGB, flip the image around y-axis for correct \n",
    "        # handedness output and process it with MediaPipe Hands.\n",
    "        results = hands.process(cv2.flip(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), 1))\n",
    "\n",
    "        annotated_image = frame.copy()\n",
    "\n",
    "        if results.multi_hand_landmarks != None:\n",
    "            history[str(frame_count)] = 1\n",
    "            annotated_image = cv2.flip(frame.copy(), 1)\n",
    "\n",
    "            #collect all thumb's location\n",
    "            temp = []\n",
    "            for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                mp_drawing.draw_landmarks(annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                temp.append(hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].y)\n",
    "            thumb_list.append(temp)\n",
    "\n",
    "            if len(results.multi_hand_landmarks) == 2:\n",
    "                history[str(frame_count)] = 2\n",
    "\n",
    "            #after processing the hand, flip back the image  \n",
    "            annotated_image = cv2.flip(annotated_image, 1)\n",
    "\n",
    "            '''\n",
    "            if len(results.multi_hand_landmarks) == 2:\n",
    "                if(results.multi_handedness[0].classification[0].label == results.multi_handedness[1].classification[0].label):\n",
    "                    print(frame_count,results.multi_handedness[0].classification[0].label)\n",
    "                else:\n",
    "                    dis = abs(right_thumb-left_thumb)\n",
    "                    thumb_dis.append(dis)\n",
    "                    img_list.append(annotated_image)\n",
    "            '''\n",
    "\n",
    "        #out.write(annotated_image)\n",
    "        img_list.append(annotated_image)\n",
    "        frame_count += 1\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "#out.release()\n",
    "\n",
    "for thumb in thumb_list:\n",
    "    if(len(thumb)) == 2:\n",
    "        dis = abs(thumb[0]-thumb[1])\n",
    "        thumb_dis.append(dis)\n",
    "plt.plot(thumb_dis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple hand\n",
    "hands = mp_hands.Hands(static_image_mode=True,max_num_hands=4,min_detection_confidence=0.5)\n",
    "\n",
    "record_left = []\n",
    "record_right =[]\n",
    "frame_count = 0\n",
    "img_list = []\n",
    "results = 0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        frame_count += 1\n",
    "\n",
    "        # Convert the BGR image to RGB, flip the image around y-axis for correct \n",
    "        # handedness output and process it with MediaPipe Hands.\n",
    "        results = hands.process(cv2.flip(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), 1))\n",
    "\n",
    "        annotated_image = frame.copy()\n",
    "\n",
    "        if results.multi_hand_landmarks != None:\n",
    "            annotated_image = cv2.flip(frame.copy(), 1)\n",
    "            for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "                mp_drawing.draw_landmarks(annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                if results.multi_handedness[idx].classification[0].label == \"Left\":\n",
    "                    thumb_y = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].y\n",
    "                    record_left.append(thumb_y)\n",
    "                elif results.multi_handedness[idx].classification[0].label == \"Right\":\n",
    "                    thumb_y = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP].y\n",
    "                    record_right.append(thumb_y)\n",
    "\n",
    "            #after processing the hand, flip back the image  \n",
    "            annotated_image = cv2.flip(annotated_image, 1)\n",
    "            img_list.append(annotated_image)\n",
    "\n",
    "        #out.write(annotated_image)\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "#out.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "axs[0].plot(record_right); axs[0].title.set_text(\"right hand\")\n",
    "axs[1].plot(record_left) ; axs[1].title.set_text(\"left hand\")\n",
    "fig.tight_layout()\n",
    "#fig.savefig(\"UPDRS_result/pronation/patient_wave.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = img_list[0][:,:128].copy()\n",
    "#do_mosaic(copy, 300, 100, 350, 650,neighbor=30)\n",
    "plt.imshow(cv2.cvtColor(copy, cv2.COLOR_BGR2RGB))\n",
    "#plt.savefig(\"hand.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = img_list[0].copy()\n",
    "cv2.putText(copy,\"action_count(right):\",(100, 70), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "  1.5, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "cv2.putText(copy,\"action_time(right):\",(100, 120), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "  1.5, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "cv2.putText(copy,\"action_count(left):\",(1250, 70), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "  1.5, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "cv2.putText(copy,\"action_time(left):\",(1250, 120), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "  1.5, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "plt.imshow(cv2.cvtColor(copy, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = record.copy()\n",
    "for i in range(1,len(smooth)):\n",
    "    smooth[i] = smooth[i-1]*0.5 + smooth[i]*0.5\n",
    "\n",
    "time = np.linspace(0,frame_count/fps,frame_count)\n",
    "plt.plot(time,smooth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot_outliers(data):\n",
    "    # finding the 1st quartile\n",
    "    q1 = np.quantile(data, 0.25)\n",
    "    \n",
    "    # finding the 3rd quartile\n",
    "    q3 = np.quantile(data, 0.75)\n",
    "    \n",
    "    # finding the iqr region\n",
    "    iqr = q3-q1\n",
    "    \n",
    "    # finding upper and lower whiskers\n",
    "    upper_bound = q3+(1.5*iqr)\n",
    "    lower_bound = q1-(1.5*iqr)\n",
    "    print(lower_bound,upper_bound)\n",
    "\n",
    "    outlier = [x for x in data if x > upper_bound]\n",
    "\n",
    "    outlier_index = []\n",
    "    for i in outlier:\n",
    "        outlier_index.append(list(data).index(i))\n",
    "\n",
    "    return outlier_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find peaks of thumb distance(pronation)\n",
    "\n",
    "smooth = thumb_dis.copy()\n",
    "middle = (max(smooth)+min(smooth))/2\n",
    "\n",
    "peaks, _ = signal.find_peaks(smooth,height=middle,prominence=0.1)\n",
    "\n",
    "\n",
    "#calculcate fist closing action time\n",
    "action_time_list = []\n",
    "action_time_list.append(peaks[0])\n",
    "for time in np.diff(peaks):\n",
    "    action_time_list.append(time)\n",
    "\n",
    "#show the peak wave\n",
    "plt.plot(smooth)\n",
    "plt.plot(peaks,np.array(smooth)[peaks],\"x\")\n",
    "#plt.savefig('UPDRS_result/pronation/self_wave.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#single hand\n",
    "#smooth = record_right.copy()\n",
    "#for i in range(1,len(smooth)):\n",
    "#    smooth[i] = smooth[i-1]*0.5 + smooth[i]*0.5\n",
    "after_filt = savgol_filter(record,31,5)\n",
    "\n",
    "middle = (max(after_filt)+min(after_filt))/2\n",
    "prominence = np.quantile(after_filt,0.75) - np.quantile(after_filt,0.25)\n",
    "\n",
    "peaks, _ = signal.find_peaks(after_filt,height=middle,prominence=prominence)\n",
    "\n",
    "fist_closing_frame = peaks\n",
    "\n",
    "#calculcate fist closing action time\n",
    "action_time_list = []\n",
    "action_time_list.append(fist_closing_frame[0])\n",
    "for time in np.diff(fist_closing_frame):\n",
    "    action_time_list.append(time)\n",
    "\n",
    "#show the peak wave\n",
    "plt.plot(after_filt)\n",
    "plt.plot(fist_closing_frame,np.array(after_filt)[fist_closing_frame],\"x\")\n",
    "#plt.savefig('wave_normal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple hand, 0 for right hand and 1 for left hand\n",
    "fist_closing_frame = []\n",
    "action_time_list = []\n",
    "fig, axs = plt.subplots(2)\n",
    "\n",
    "for hand in range(2):\n",
    "    smooth = []\n",
    "    label = \"\"\n",
    "    if hand == 0:\n",
    "        smooth = record_right.copy()\n",
    "        label = \"right hand\"\n",
    "    else:\n",
    "        smooth = record_left.copy()\n",
    "        label = \"left hand\"\n",
    "\n",
    "    for i in range(1,len(smooth)):\n",
    "        smooth[i] = smooth[i-1]*0.5 + smooth[i]*0.5\n",
    "    middle = (max(smooth)+min(smooth))/2\n",
    "\n",
    "    peaks, _ = signal.find_peaks(smooth,height=middle,prominence=0.05)\n",
    "\n",
    "    fist_closing_frame.append(peaks)\n",
    "\n",
    "    #calculcate fist closing action time\n",
    "    temp = []\n",
    "    temp.append(peaks[0])\n",
    "    for time in np.diff(peaks):\n",
    "        temp.append(time)\n",
    "    action_time_list.append(temp)\n",
    "\n",
    "    #show the peak wave\n",
    "    #plt.plot(smooth)\n",
    "    #plt.plot(peaks,np.array(smooth)[peaks],\"x\")\n",
    "    #plt.savefig('wave_normal.png')\n",
    "\n",
    "    axs[hand].plot(smooth) \n",
    "    axs[hand].plot(peaks,np.array(smooth)[peaks],\"x\")\n",
    "    axs[hand].title.set_text(label)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('two-hands_wave.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write result to video (with image loss)(pronation)\n",
    "action_count = 0\n",
    "action_time = 0\n",
    "two_hands_frame = 0\n",
    "for i in range(len(img_list)):\n",
    "    if history[str(i)] == 2:\n",
    "        if len(peaks) > action_count and two_hands_frame == peaks[action_count]:\n",
    "            action_time = action_time_list[action_count]\n",
    "            action_count += 1\n",
    "        two_hands_frame += 1\n",
    "\n",
    "    cv2.putText(img_list[i],f'action_count:{action_count}',(10, 70), cv2.FONT_HERSHEY_SIMPLEX,1.5, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "    cv2.putText(img_list[i],f'action_time:{action_time/fps:.2f}s',(10, 120), cv2.FONT_HERSHEY_SIMPLEX,1.5, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "    out.write(img_list[i])\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write result to video (single hand)\n",
    "action_count = 0\n",
    "action_time = 0\n",
    "for i in range(len(img_list)):\n",
    "    if( len(fist_closing_frame) > action_count and i == fist_closing_frame[action_count]):\n",
    "        action_time = action_time_list[action_count]\n",
    "        action_count += 1\n",
    "    cv2.putText(img_list[i],f'action_count:{action_count}',(10, 70), cv2.FONT_HERSHEY_SIMPLEX,1.5, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "    cv2.putText(img_list[i],f'action_time:{action_time/fps:.2f}s',(10, 120), cv2.FONT_HERSHEY_SIMPLEX,1.5, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "    out.write(img_list[i])\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write result to video (multiple hand)\n",
    "action_count = [0,0]\n",
    "action_time = [0,0]\n",
    "for i in range(len(img_list)):\n",
    "    label = \"\"\n",
    "    text_loc_x = 0\n",
    "    for hand in range(2):\n",
    "        if hand == 0:\n",
    "            label = \"right\"\n",
    "            text_loc_x = 100\n",
    "        else:\n",
    "            label = \"left\"\n",
    "            text_loc_x = 1250\n",
    "\n",
    "        if( (len(fist_closing_frame[hand]) > action_count[hand]) and i == fist_closing_frame[hand][action_count[hand]]):\n",
    "            action_time[hand] = action_time_list[hand][action_count[hand]]\n",
    "            action_count[hand] += 1\n",
    "        cv2.putText(img_list[i],f'action_count({label}):{action_count[hand]}',(text_loc_x, 70), cv2.FONT_HERSHEY_SIMPLEX,1.5, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "        cv2.putText(img_list[i],f'action_time({label}):{action_time[hand]/fps:.2f}s',(text_loc_x, 120), cv2.FONT_HERSHEY_SIMPLEX,1.5, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "        do_mosaic(img_list[i], 700, 50, 500, 500,neighbor=30)\n",
    "    out.write(img_list[i])\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (1,2)\n",
    "a = (3,4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f35382d907cb9c24a78f87fdb40cfd6f3acc0f7e4b3752e533ecdd62cd27e97f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('py3.9': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
